{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation 1\n",
    "This code aims to clean the data of bike count with its Monitoring Locations in London\n",
    "\n",
    "How it works:\n",
    "1. Check the list of Monitoring Locations\n",
    "2. Check the sample of bike counts raw data\n",
    "3. Read all the bike counts raw data, merge the rows into hourly data\n",
    "4. Join to list of monitoring locations to get the column of borough, funtional, and coordinate\n",
    "5. Extract List of dates for rainfall data (Data_Preparation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Package\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Monitoring Locations / Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    UnqID                            Location         Borough  Functional  \\\n",
      "0  ML0001  Millbank (south of Thorney Street)     Westminster  01 Central   \n",
      "1  ML0002                         Bishopsgate  City of London  01 Central   \n",
      "2  ML0003                    Southwark Bridge       Southwark  01 Central   \n",
      "3  ML0004               Southwark Bridge Road       Southwark  01 Central   \n",
      "4  ML0005                       Tooley Street       Southwark  01 Central   \n",
      "\n",
      "   Road_type Old site I    Easting   Northing   Latitude  Longitude  \n",
      "0  01 A Road   CENCY001  530251.49  178742.45  51.492628  -0.125204  \n",
      "1  01 A Road   CENCY002  533362.68  181824.45  51.519599  -0.079254  \n",
      "2  01 A Road   CENCY003  532334.06  180520.37  51.508123  -0.094551  \n",
      "3  01 A Road   CENCY004  532052.50  179677.64  51.500613  -0.098927  \n",
      "4  01 A Road   CENCY005  533031.59  180213.46  51.505200  -0.084629  \n"
     ]
    }
   ],
   "source": [
    "monitor_loc = pd.read_csv('bike_count/processed/Point_All_cleaned.csv', sep=',')\n",
    "\n",
    "# Rename \n",
    "monitor_loc.rename(columns={'Site ID': 'UnqID'}, inplace=True)\n",
    "monitor_loc.rename(columns={'Easting (U': 'Easting'}, inplace=True)\n",
    "monitor_loc.rename(columns={'Northing (': 'Northing'}, inplace=True)\n",
    "\n",
    "print (monitor_loc.head())\n",
    "\n",
    "# Note: The Road Type is coming from the raw data: not complete\n",
    "# Note: Cleaned means raw data (Site ID) already overlayed with the boundary with boundary shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Bike Counts Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Check 1 Sample to aggregate/ sum hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Year   UnqID        Date Weather      Time  \\\n",
      "0      2015 Q2 spring (Apr-Jun)  ML0001  04/06/2015     Dry  06:00:00   \n",
      "1      2015 Q2 spring (Apr-Jun)  ML0001  04/06/2015     Dry  06:15:00   \n",
      "2      2015 Q2 spring (Apr-Jun)  ML0001  04/06/2015     Dry  06:30:00   \n",
      "3      2015 Q2 spring (Apr-Jun)  ML0001  04/06/2015     Dry  06:45:00   \n",
      "4      2015 Q2 spring (Apr-Jun)  ML0001  04/06/2015     Dry  07:00:00   \n",
      "...                         ...     ...         ...     ...       ...   \n",
      "51451  2015 Q2 spring (Apr-Jun)  ML0104  15/06/2015     Dry  20:45:00   \n",
      "51452  2015 Q2 spring (Apr-Jun)  ML0104  15/06/2015     Dry  21:00:00   \n",
      "51453  2015 Q2 spring (Apr-Jun)  ML0104  15/06/2015     Dry  21:15:00   \n",
      "51454  2015 Q2 spring (Apr-Jun)  ML0104  15/06/2015     Dry  21:30:00   \n",
      "51455  2015 Q2 spring (Apr-Jun)  ML0104  15/06/2015     Dry  21:45:00   \n",
      "\n",
      "           Day Round         Dir Path              Mode  Count  \n",
      "0      Weekday     A  Northbound  NaN    Private cycles      3  \n",
      "1      Weekday     A  Northbound  NaN    Private cycles     10  \n",
      "2      Weekday     A  Northbound  NaN    Private cycles     18  \n",
      "3      Weekday     A  Northbound  NaN    Private cycles     39  \n",
      "4      Weekday     A  Northbound  NaN    Private cycles     65  \n",
      "...        ...   ...         ...  ...               ...    ...  \n",
      "51451  Weekday     A  Southbound  NaN  Cycle hire bikes      2  \n",
      "51452  Weekday     A  Southbound  NaN  Cycle hire bikes      2  \n",
      "51453  Weekday     A  Southbound  NaN  Cycle hire bikes      2  \n",
      "51454  Weekday     A  Southbound  NaN  Cycle hire bikes      3  \n",
      "51455  Weekday     A  Southbound  NaN  Cycle hire bikes      2  \n",
      "\n",
      "[51456 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# check 1 sample of the raw data: year 2015 only central\n",
    "y2015_central = pd.read_csv('bike_count/raw_data_tfl/2015 Q2 spring (Apr-Jun)-Central.csv', sep=',')\n",
    "print (y2015_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       UnqID       Date  Hour  Count\n",
      "0     ML0001 2015-06-04     6     86\n",
      "1     ML0001 2015-06-04     7    491\n",
      "2     ML0001 2015-06-04     8    853\n",
      "3     ML0001 2015-06-04     9    385\n",
      "4     ML0001 2015-06-04    14     88\n",
      "...      ...        ...   ...    ...\n",
      "3374  ML0208 2015-04-30    17     23\n",
      "3375  ML0208 2015-04-30    18     36\n",
      "3376  ML0208 2015-04-30    19     18\n",
      "3377  ML0208 2015-04-30    20     27\n",
      "3378  ML0208 2015-04-30    21     18\n",
      "\n",
      "[3379 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert \"Date\" column to datetime format\n",
    "y2015_central['Date'] = pd.to_datetime(y2015_central['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Convert \"Time\" column to datetime format\n",
    "y2015_central['Time'] = pd.to_datetime(y2015_central['Time'], format='%H:%M:%S')\n",
    "\n",
    "# Extract the hour from the \"Time\" column\n",
    "y2015_central['Hour'] = y2015_central['Time'].dt.hour\n",
    "\n",
    "# Group by \"Date,\" \"UnqID,\" and \"Hour\" and aggregate the \"Count\" column\n",
    "hourly_data_y2015_central = y2015_central.groupby(['UnqID', 'Date', 'Hour'])['Count'].sum().reset_index()\n",
    "\n",
    "# Display the aggregated hourly data\n",
    "print(hourly_data_y2015_central)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For notes: Hour 6 means 06:00 - 06:59\n",
    "\n",
    "Now, try to iterate to all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific file names to process\n",
    "file_names = [\n",
    "    \"2015 Q2 spring (Apr-Jun)-Central.csv\",\n",
    "    \"2015 Q2 spring (Apr-Jun)-Cycleways.csv\",\n",
    "    \"2015 Q2 spring (Apr-Jun)-Inner.csv\",\n",
    "    \"2015 Q2 spring (Apr-Jun)-Outer.csv\",\n",
    "    \"2016 Q2 spring (Apr-Jun)-Central.csv\",\n",
    "    \"2016 Q2 spring (Apr-Jun)-Cycleways.csv\",\n",
    "    \"2016 Q2 spring (Apr-Jun)-Inner.csv\",\n",
    "    \"2016 Q2 spring (Apr-Jun)-Outer.csv\",\n",
    "    \"2017 Q2 spring (Apr-Jun)-Central.csv\",\n",
    "    \"2017 Q2 spring (Apr-Jun)-Cycleways.csv\",\n",
    "    \"2017 Q2 spring (Apr-Jun)-Inner.csv\",\n",
    "    \"2017 Q2 spring (Apr-Jun)-Outer.csv\",\n",
    "    \"2018 Q2 spring (Apr-Jun)-Central.csv\",\n",
    "    \"2018 Q2 spring (Apr-Jun)-Cycleways.csv\",\n",
    "    \"2018 Q2 spring (Apr-Jun)-Inner.csv\",\n",
    "    \"2018 Q2 spring (Apr-Jun)-Outer.csv\",\n",
    "    \"2019 Q2 spring (Apr-Jun)-Central.csv\",\n",
    "    \"2019 Q2 spring (Apr-Jun)-Cycleways.csv\",\n",
    "    \"2019 Q2 spring (Apr-Jun)-Inner.csv\",\n",
    "    \"2019 Q2 spring (Apr-Jun)-Outer.csv\",\n",
    "    \"2020 Q2 spring (synthetic)-Central.csv\",\n",
    "    \"2020 Q2 spring (synthetic)-Inner.csv\",\n",
    "    \"2020 Q2 spring (synthetic)-Outer.csv\",\n",
    "    \"2021 Q2 spring (Apr-Jun)-Central.csv\",\n",
    "    \"2021 Q2 spring (Apr-Jun)-Cycleways.csv\",\n",
    "    \"2021 Q2 spring (Apr-Jun)-Inner.csv\",\n",
    "    \"2021 Q2 spring (Apr-Jun)-Outer.csv\",\n",
    "    \"2022 W1 spring-Central.csv\",\n",
    "    \"2022 W1 spring-Cycleways.csv\",\n",
    "    \"2022 W1 spring-Inner-Part1.csv\",\n",
    "    \"2022 W1 spring-Inner-Part2.csv\",\n",
    "    \"2022 W1 spring-Outer.csv\",\n",
    "    \"2023 W1 spring-Central.csv\",\n",
    "    \"2023 W1 spring-Cycleways.csv\",\n",
    "    \"2023 W1 spring-Inner-Part1.csv\",\n",
    "    \"2023 W1 spring-Inner-Part2.csv\",\n",
    "    \"2023 W1 spring-Outer.csv\",\n",
    "    \"2024 W1 spring-Central.csv\",\n",
    "    \"2024 W1 spring-Cycleways.csv\",\n",
    "    \"2024 W1 spring-Inner-Part1.csv\",\n",
    "    \"2024 W1 spring-Inner-Part2.csv\",\n",
    "    \"2024 W1 spring-Outer.csv\"\n",
    "]\n",
    "\n",
    "# Year 2020 does not have Cycleways data (incomplete), and the data is synthetic, should I still used the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_25484\\2768977537.py:13: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep=',')\n",
      "C:\\Users\\arthu\\AppData\\Local\\Temp\\ipykernel_25484\\2768977537.py:13: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, sep=',')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UnqID  wave  Year       Date  Hour  Count  \\\n",
      "0       ML0001  2015  2015 2015-06-04     6     86   \n",
      "1       ML0001  2015  2015 2015-06-04     7    491   \n",
      "2       ML0001  2015  2015 2015-06-04     8    853   \n",
      "3       ML0001  2015  2015 2015-06-04     9    385   \n",
      "4       ML0001  2015  2015 2015-06-04    14     88   \n",
      "...        ...   ...   ...        ...   ...    ...   \n",
      "249492  ML1451  2024  2024 2024-04-18    17     29   \n",
      "249493  ML1451  2024  2024 2024-04-18    18     47   \n",
      "249494  ML1451  2024  2024 2024-04-18    19     35   \n",
      "249495  ML1451  2024  2024 2024-04-18    20     11   \n",
      "249496  ML1451  2024  2024 2024-04-18    21      6   \n",
      "\n",
      "                                  SourceFile  \n",
      "0       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "1       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "2       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "3       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "4       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "...                                      ...  \n",
      "249492              2024 W1 spring-Outer.csv  \n",
      "249493              2024 W1 spring-Outer.csv  \n",
      "249494              2024 W1 spring-Outer.csv  \n",
      "249495              2024 W1 spring-Outer.csv  \n",
      "249496              2024 W1 spring-Outer.csv  \n",
      "\n",
      "[249497 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Base path of the files\n",
    "base_path = 'bike_count/raw_data_tfl'\n",
    "\n",
    "# List to store processed data\n",
    "all_hourly_data = []\n",
    "\n",
    "# Loop through the specified files\n",
    "for file_name in file_names:\n",
    "    # Construct the full file path\n",
    "    file_path = f\"{base_path}\\\\{file_name}\"\n",
    "    \n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "    # Extract the year part from the \"year\" column and store it in a new column \"wave\"\n",
    "    data['wave'] = data['Year'].str.extract(r'(\\d{4})')\n",
    "\n",
    "    # Exclude rows where \"Mode\" is \"E-scooters\" or \"Pedestrian\"\n",
    "    data = data[~data['Mode'].isin(['E-scooters', 'Pedestrians'])]\n",
    "    \n",
    "    # Convert \"Date\" column to datetime format\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')\n",
    "\n",
    "    # Extract the year from \"Date\" column\n",
    "    data['Year'] = data['Date'].dt.year\n",
    "    \n",
    "    # Convert \"Time\" column to datetime format\n",
    "    data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S')\n",
    "    \n",
    "    # Extract the hour from the \"Time\" column\n",
    "    data['Hour'] = data['Time'].dt.hour\n",
    "    \n",
    "    # Group by \"Date\", \"UnqID\", and \"Hour\" and aggregate the \"Count\" column\n",
    "    # Weather is not added, because the same hour in one day could have 2 types of weather description. It will be replaced by continues data later on.  \n",
    "    hourly_data = data.groupby(['UnqID', 'wave', 'Year', 'Date', 'Hour'])['Count'].sum().reset_index()\n",
    "    \n",
    "    # Add a column to indicate the source file\n",
    "    hourly_data['SourceFile'] = file_name\n",
    "    \n",
    "    # Append the processed data to the list\n",
    "    all_hourly_data.append(hourly_data)\n",
    "\n",
    "# Combine all the processed data into a single DataFrame\n",
    "final_hourly_data = pd.concat(all_hourly_data, ignore_index=True)\n",
    "\n",
    "# Display the aggregated hourly data\n",
    "print(final_hourly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UnqID  wave  Year       Date  Hour  Count  \\\n",
      "0       ML0001  2015  2015 2015-06-04     6     86   \n",
      "1       ML0001  2015  2015 2015-06-04     7    491   \n",
      "2       ML0001  2015  2015 2015-06-04     8    853   \n",
      "3       ML0001  2015  2015 2015-06-04     9    385   \n",
      "4       ML0001  2015  2015 2015-06-04    14     88   \n",
      "...        ...   ...   ...        ...   ...    ...   \n",
      "249492  ML1451  2024  2024 2024-04-18    17     29   \n",
      "249493  ML1451  2024  2024 2024-04-18    18     47   \n",
      "249494  ML1451  2024  2024 2024-04-18    19     35   \n",
      "249495  ML1451  2024  2024 2024-04-18    20     11   \n",
      "249496  ML1451  2024  2024 2024-04-18    21      6   \n",
      "\n",
      "                                  SourceFile  \n",
      "0       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "1       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "2       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "3       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "4       2015 Q2 spring (Apr-Jun)-Central.csv  \n",
      "...                                      ...  \n",
      "249492              2024 W1 spring-Outer.csv  \n",
      "249493              2024 W1 spring-Outer.csv  \n",
      "249494              2024 W1 spring-Outer.csv  \n",
      "249495              2024 W1 spring-Outer.csv  \n",
      "249496              2024 W1 spring-Outer.csv  \n",
      "\n",
      "[249497 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'wave' and 'Year' are different\n",
    "mismatched_rows = final_hourly_data[final_hourly_data['wave'] != final_hourly_data['Year']]\n",
    "\n",
    "# Display the mismatched rows\n",
    "print(mismatched_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the aggregated data to a new CSV file\n",
    "final_hourly_data.to_csv('bike_count/processed/aggregated_hourly_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [UnqID, wave, Year, Date, Hour, Count, SourceFile]\n",
      "Index: []\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = final_hourly_data[final_hourly_data.duplicated()]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(\"Duplicate rows:\")\n",
    "print(duplicate_rows)\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicate_rows.shape[0]\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# If needed, save duplicate rows to a CSV file for review\n",
    "#if num_duplicates > 0:\n",
    "#    duplicate_rows.to_csv('duplicate_rows.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join to Monitoring Locations/ Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         UnqID  wave  Year       Date  Hour  Count      Borough  Functional  \\\n",
      "0       ML0001  2015  2015 2015-06-04     6     86  Westminster  01 Central   \n",
      "1       ML0001  2015  2015 2015-06-04     7    491  Westminster  01 Central   \n",
      "2       ML0001  2015  2015 2015-06-04     8    853  Westminster  01 Central   \n",
      "3       ML0001  2015  2015 2015-06-04     9    385  Westminster  01 Central   \n",
      "4       ML0001  2015  2015 2015-06-04    14     88  Westminster  01 Central   \n",
      "...        ...   ...   ...        ...   ...    ...          ...         ...   \n",
      "249492  ML1451  2024  2024 2024-04-18    17     29        Brent    03 Outer   \n",
      "249493  ML1451  2024  2024 2024-04-18    18     47        Brent    03 Outer   \n",
      "249494  ML1451  2024  2024 2024-04-18    19     35        Brent    03 Outer   \n",
      "249495  ML1451  2024  2024 2024-04-18    20     11        Brent    03 Outer   \n",
      "249496  ML1451  2024  2024 2024-04-18    21      6        Brent    03 Outer   \n",
      "\n",
      "         Latitude  Longitude    Easting   Northing  \n",
      "0       51.492628  -0.125204  530251.49  178742.45  \n",
      "1       51.492628  -0.125204  530251.49  178742.45  \n",
      "2       51.492628  -0.125204  530251.49  178742.45  \n",
      "3       51.492628  -0.125204  530251.49  178742.45  \n",
      "4       51.492628  -0.125204  530251.49  178742.45  \n",
      "...           ...        ...        ...        ...  \n",
      "249492  51.557795  -0.232307  522641.94  185804.92  \n",
      "249493  51.557795  -0.232307  522641.94  185804.92  \n",
      "249494  51.557795  -0.232307  522641.94  185804.92  \n",
      "249495  51.557795  -0.232307  522641.94  185804.92  \n",
      "249496  51.557795  -0.232307  522641.94  185804.92  \n",
      "\n",
      "[249497 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Perform a left join between hourly_data and monitor_loc on the \"UnqID\" column\n",
    "merged_hourly_data = pd.merge(final_hourly_data, monitor_loc, on='UnqID', how='left')\n",
    "\n",
    "# Reorder the columns to make 'UnqID' the first column\n",
    "merged_hourly_data = merged_hourly_data[['UnqID', 'wave', 'Year', 'Date', 'Hour', 'Count', 'Borough', 'Functional', 'Latitude', 'Longitude', 'Easting', 'Northing']]\n",
    "\n",
    "# Display the merged table\n",
    "print(merged_hourly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame to a CSV file in your project directory\n",
    "merged_hourly_data.to_csv('bike_count/processed/merged_hourly_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully split into 11 files and saved to bike_count/processed/yearly_data.\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'bike_count/processed/yearly_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each unique year in the 'Year' column\n",
    "unique_years = merged_hourly_data['Year'].unique()\n",
    "\n",
    "for year in unique_years:\n",
    "    # Filter the DataFrame for the current year\n",
    "    yearly_data = merged_hourly_data[merged_hourly_data['Year'] == year]\n",
    "    \n",
    "    # Create a file name for the current year\n",
    "    file_name = f'{output_dir}/bike_count_{year}.csv'\n",
    "    \n",
    "    # Save the filtered data to a CSV file\n",
    "    yearly_data.to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"Data successfully split into {len(unique_years)} files and saved to {output_dir}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract List of Dates for Rainfall Data (Data_Preparation_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      wave  Year       Date\n",
      "0     2015  2014 2014-09-01\n",
      "1     2015  2014 2014-09-02\n",
      "2     2015  2014 2014-09-08\n",
      "3     2015  2014 2014-09-09\n",
      "4     2015  2014 2014-09-10\n",
      "...    ...   ...        ...\n",
      "1039  2024  2024 2024-07-11\n",
      "1040  2024  2024 2024-07-16\n",
      "1041  2024  2024 2024-07-17\n",
      "1042  2024  2024 2024-07-18\n",
      "1043  2024  2024 2024-07-23\n",
      "\n",
      "[1044 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract unique dates\n",
    "unique_dates_per_year = merged_hourly_data[['wave', 'Year', 'Date']].drop_duplicates().sort_values(by=['wave', 'Year', 'Date'])\n",
    "\n",
    "# Reset the index for a clean output\n",
    "unique_dates_per_year = unique_dates_per_year.reset_index(drop=True)\n",
    "\n",
    "# Display the vertical list\n",
    "print(unique_dates_per_year)\n",
    "\n",
    "# Optionally save to a CSV file\n",
    "unique_dates_per_year.to_csv('bike_count/processed/unique_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
